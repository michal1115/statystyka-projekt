---
title: "Regresja"
author: "Rafał Lisak & Michał Grzybek"
date: "6 06 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


#install.packages("leaps")
#install.packages("glmnet")
#install.packages("spline")
#install.packages("tree")
#install.packages('lattice')
#install.packages('ggplot2')
#install.packages('gmodels')
#install.packages('psych')
#install.packages("rpart.plot")
library(rpart.plot)
library(caret)
library(class)
library(e1071)
library(gmodels) 
library(caret)
library(FNN)
library(leaps)
library(glmnet)
library(splines)
library(MASS)
library(tree)
library(ISLR)
library(psych)
library(dplyr)
```

```{r}
dataset <- read.csv("student-por.csv", sep = ';', header = TRUE)
dataset <- subset(dataset, select = -c(G1, G2))#wyrzucamy oceny semestralny, gdyż w oczywisty sposób są najbardziej znaczące dla oceny końcowej
#dataset <- subset(dataset, select = -c(school))#pozbywamy się również szkoły, gdyż w zbiorze danych są tylko dwie
```

```{r}
data_raw<-read.csv('student-por.csv', sep = ';',header = TRUE,stringsAsFactors = TRUE)
head(data_raw)
```

```{r}
data_raw <- data_raw  %>% dplyr::select(-G1)
data_raw <- data_raw  %>% dplyr::select(-G2)
data_classification <- data_raw
str(data_classification)
```

```{r}
#Współczynniki numeryczne możemy przeskalować 
data_classification[, c("age", "Medu", "Fedu", "traveltime", "studytime", "failures", "famrel", "freetime", "goout", "Dalc", "Walc", "health", "absences", "G3")] <- scale(data_classification[, c("age", "Medu", "Fedu", "traveltime", "studytime", "failures", "famrel", "freetime", "goout", "Dalc", "Walc", "health", "absences", "G3")])
```


```{r}
#Współczynniki odpowiadające tak lub nie możemy łatwo zamienić na wartości binarne
data_classification$schoolsup <- ifelse(data_classification$schoolsup == "yes", 1, 0)
data_classification$famsup <- ifelse(data_classification$famsup == "yes", 1, 0)
data_classification$paid <- ifelse(data_classification$paid == "yes", 1, 0)
data_classification$activities <- ifelse(data_classification$activities == "yes", 1, 0)
data_classification$nursery <- ifelse(data_classification$nursery == "yes", 1, 0)
data_classification$higher <- ifelse(data_classification$higher == "yes", 1, 0)
data_classification$internet <- ifelse(data_classification$internet == "yes", 1, 0)
data_classification$romantic <- ifelse(data_classification$romantic == "yes", 1, 0)
```

```{r}
#Współczynniki posiadające dwa poziomy nie możemy zmienić na binarne ponieważ tracilibyśmy informacje i mobilbyśmy uzyskać błędne relacje. Rozdzielamy je na osobne kolumny.
data_classification$school <- dummy.code(data_classification$school)
data_classification$address <- dummy.code(data_classification$address)
data_classification$famsize <- dummy.code(data_classification$famsize)
data_classification$Pstatus <- dummy.code(data_classification$Pstatus)
data_classification$sex <- dummy.code(data_classification$sex)
```

```{r}
#Współczynniki posiadające więcej niż dwa poziomy też rozdzielamy na binarne kolumny ale przedtym zmieniamy ich nazwy żeby się nie powtarzały.
Fjob <- as.data.frame(dummy.code(data_classification$Fjob))
Mjob <- as.data.frame(dummy.code(data_classification$Mjob))
reason <- as.data.frame(dummy.code(data_classification$reason))
guardian <- as.data.frame(dummy.code(data_classification$guardian))
```

```{r}
Fjob <- rename(Fjob, services_Fjob = services)
Fjob <- rename(Fjob, at_home_Fjob = at_home)
Fjob <- rename(Fjob, teacher_Fjob = teacher)
Mjob <- rename(Mjob, services_Mjob = services)
Mjob <- rename(Mjob, at_home_Mjob = at_home)
Mjob <- rename(Mjob, teacher_Mjob = teacher)
Fjob <- rename(Fjob, other_Fjob = other)
Fjob <- rename(Fjob, health_Fjob = health)
Mjob <- rename(Mjob, other_Mjob = other)
Mjob <- rename(Mjob, health_Mjob = health)
reason <- rename(reason, other_reason = other)

guardian <- rename(guardian, other_guardian = other)
data_classification <- cbind(data_classification, Fjob, Mjob, guardian, reason)
data_classification <- data_classification %>% dplyr::select(-one_of(c("Fjob","Mjob", "guardian", "reason")))
```

```{r}
sex_outcome <- data_raw %>% dplyr::select(sex) # zbiór, który będziemy przewidywać
data_classification_without_sex <- data_classification %>% dplyr::select(-sex) # usuwamy kolumnę, którą będziemy przewidywać
set.seed(1234) 

# 75% datasetu jako zbiór treningowy
smp_size <- floor(0.75 * nrow(data_classification_without_sex))

train_ind <- sample(seq_len(nrow(data_classification_without_sex)), size = smp_size)

# tworzymy test i train sety
class_pred_train <- data_classification_without_sex[train_ind, ]
class_pred_test <- data_classification_without_sex[-train_ind, ]
sex_outcome_train <- sex_outcome[train_ind, ]
sex_outcome_test <- sex_outcome[-train_ind, ]
sex_pred_knn <- knn(train = class_pred_train, test = class_pred_test, cl = sex_outcome_train, k=17)

confusionMatrix(sex_pred_knn,  as.factor(sex_outcome_test))
```

```{r}
library(rpart) 
smp_size <- floor(0.75 * nrow(dataset))
set.seed(124)
train_ind <- sample(seq_len(nrow(dataset)), size = smp_size)

# towrzymy zbiory treningowe
train <- dataset[train_ind, ]
test <- dataset[-train_ind, ]
#Na podstawie szkoły do której chodzą uczniowie oraz czasu jaki zajumje im podróż do szkoły przewidujemy czy mieszkają w mieście czy na wsi
tree1 <- rpart(address ~ school + traveltime, data = train, method="class")
summary(tree1)
```

```{r}
rpart.plot(tree1)
pred1 <- predict(tree1,newdata=test,type="class")
```

```{r}
reference <- test$address
u <- union(pred1, reference)
t <- table(factor(pred1, u), factor(reference, u))
confusionMatrix(t)
```
##LDA

```{r league-lda}
legue_lda <- list()
legue_lda$fit <- lda(address ~ school + traveltime, data = train)
legue_lda$predicted <- predict(legue_lda$fit, test)
confusionMatrix(table(legue_lda$predicted$class, test$address))
```


##QDA
```{r}
dir_qda <- list()
dir_qda$fit <- qda(address ~ school + traveltime, data = train)
dir_qda$predicted <- predict(dir_qda$fit, test)
confusionMatrix(table(dir_qda$predicted$class, test$address))
```

```{r}
linear_fit <- lm(G3 ~ ., data = dataset)
summary(linear_fit)
```

```{r}
dataset_bs <- regsubsets(G3 ~ ., data=dataset, nvmax = 15)
summary(dataset_bs)
```

```{r}
cp_norms = summary(dataset_bs)$cp
plot(cp_norms)
```

```{r}
bayes_norms = summary(dataset_bs)$bic
plot(bayes_norms)
coef(dataset_bs, id = 5)

```

```{r}
plot(summary(dataset_bs)$adjr2)
```

```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  model_formula <- as.formula(object$call[[2]])
  mat <- model.matrix(model_formula, newdata)
  coefs <- coef(object, id = id)
  mat[, names(coefs)] %*% coefs
}

prediction_error <- function(i, model, subset) {
  pred <- predict(model, dataset[subset,], id = i)
  mean((dataset$G3[subset] - pred)^2)
}
```

Szukanie optymalnej liczby predyktorów przy użyciu walidacji krzyżowej:
```{r}
k <- 10
max_predictors <- 10

folds <- sample(1:k, nrow(dataset), replace=TRUE)#dzielimy nasz zbior na podzbiory do k-krotnej walidacji krzyzowej
val_err <- NULL

for (j in 1:k) {
  fit_bs <- regsubsets(G3 ~ ., data=dataset[folds!=j,], nvmax=max_predictors)
  err <- sapply(1:max_predictors, prediction_error, model = fit_bs, subset = (folds == j))
  val_err <- rbind(val_err, err)
}
colMeans(val_err)
```
Na podstawie tego, oraz wyników Bayesowskiego Kryterium Informacyjnego, postanowiliśmy wziąć do obliczeń 5 predyktorów.
Były to Medu, studytime, failures, higheryes oraz Dalc

```{r}
summary(lm(G3 ~ failures + higher + studytime + Dalc + Medu, data = dataset))
```
Jak widać, wszystkie predyktory wykazały bardzo dużą wartość P-value.

Konwersja predyktorów jakościowych na ich odpowiedniki liczbowe:
```{r}
dataset$higherYes <- ifelse(dataset$higher=="yes", 1, 0)

failures_max_degree <- length(unique(dataset$failures)) - 1
higherYes_max_degree <- length(unique(dataset$higherYes)) - 1
studtytime_max_degree <- length(unique(dataset$studytime)) - 1
Dalc_max_degree <- length(unique(dataset$Dalc)) - 1
Medu_max_degree <- length(unique(dataset$Medu)) - 1
```
Postanowiliśmy sprawdzić jaki może być optymalny stopień wielomianu dla poszczególnych predyktorów:
```{r}
set.seed(2)
k <- 5
folds <- sample(1:k, nrow(dataset), replace=TRUE)

results <- data.frame(failures_degree=numeric(), higherYes_degree=numeric(), studytime_degree=numeric(), Dalc_degree=numeric(), Medu_degree=numeric(), test_error=numeric())

for (d_1 in 1:failures_max_degree){
  for (d_2 in 1:higherYes_max_degree){
    for (d_3 in 1:studtytime_max_degree){
      for (d_4 in 1:Dalc_max_degree){
        for (d_5 in 1:Medu_max_degree){
          mean_cv_pred_err <- c()
          for (cv_iter in 1:k){
            train <- dataset[folds != cv_iter,]
            test <- dataset[folds == cv_iter,]
            linear_fit <- lm(G3 ~ poly(failures, d_1) + poly(higherYes, d_2) + poly(studytime, d_3) + poly(Dalc, d_4) 
                       + poly(Medu, d_5), data = train)
            mean_cv_pred_err <- c(mean_cv_pred_err, c(mean((test$G3 - predict(linear_fit, test))^2)))
          }
          results[nrow(results) +1,] = c(d_1, d_2, d_3, d_4, d_5, mean(mean_cv_pred_err))
        }
      }
    }
  }
}
```

```{r}
results[order(results$test_error, decreasing = FALSE),]
```

Z powyższej tabeli zawierającej posortowane błędy względem stopnia wielomianu wynika, że stopień studytime oraz Dalc nie ma aż tak duzego znaczenia, dlatego postanawiamy tutaj nie komplikować modelu, gdyż może to wpłynąć na zwiększenie wariancji. Postanawiamy wykorzystać poniższe stopnie wielomianóW:
failures: 2
higherYes: 1
studytime: 1
Dalc: 1
Medu: 1

```{r}
lm_fit <- lm(G3 ~ poly(failures, 2) + higherYes + studytime + Dalc + Medu, data = dataset)
summary(lm_fit)
```
Otrzymaliśmy błąd testowy o wartości:
```{r}
set.seed(2)
k <- 5
folds <- sample(1:k, nrow(dataset), replace=TRUE)

mean_cv_pred_err <- c()
for (cv_iter in 1:k){
  train <- dataset[folds != cv_iter,]
  test <- dataset[folds == cv_iter,]
  linear_fit <- lm(G3 ~ poly(failures, 2) + higherYes + studytime + Dalc + Medu, data = train)
  mean_cv_pred_err <- c(mean_cv_pred_err, c(mean((test$G3 - predict(linear_fit, test))^2)))
}
mean(mean_cv_pred_err)
```

Dla otrzymanych wartości współczynników postanowiliśmy przeprowadzić również regularyzację:
Rozpoczęliśmy od szukania optymalnego lambda dla regresji grzbietowej przy użyciu k-krotnej walidacji krzyżowej:
```{r}
X <- model.matrix(G3 ~ poly(failures, 2) + higherYes + studytime + Dalc + Medu, data = dataset)[, -1]
y <- dataset$G3

set.seed(2)
k <- 5

folds <- sample(1:k, nrow(X), replace=TRUE)

lambdas = c()
for (i in 1:k){
  cv_out <- cv.glmnet(X[folds!=i,], y[folds!=i], alpha = 0)
  lambdas = c(lambdas, cv_out$lambda.min)
  plot(cv_out)
}
opt_lambda <- mean(lambdas)
opt_lambda
```
Optymalna lambda wyniosła więc 0.3003844.

Bład testowy dla otrzymanej lambdy:
```{r}
set.seed(2)
lambda_grid <- 10^seq(10, -2, length.out = 100)
fit_ridge_full <- glmnet(X, y, alpha = 0)

k <- 5
folds <- sample(1:k, nrow(X), replace=TRUE)

test_errors = c()
for (i in 1:k){
  pred_ridge_opt <- predict(fit_ridge_full, s = opt_lambda, newx = X[folds==i,])
  mean_pred_error <- mean((pred_ridge_opt - y[folds==i])^2)
  test_errors = c(test_errors, mean_pred_error)
}
mean(test_errors)
```
Błąd spadł

```{r}
predict(fit_ridge_full, s = cv_out$lambda.min, type = "coefficients")
```
Próba regulacryzacji pozwoliła nam jeszcze bardziej zmniejszyć błąd testowy.

Postanowiliśmy również porównać nasz model z modelem czysto liniowym:
```{r}
set.seed(2)
k <- 5
folds <- sample(1:k, nrow(dataset), replace=TRUE)

lower_stds <- c()
mean_cv_pred_err <- c()
higher_stds <- c()
for (cv_iter in 1:k){
  train <- dataset[folds != cv_iter,]
  test <- dataset[folds == cv_iter,]
  
  model <- lm(G3 ~ failures + higherYes + studytime + Dalc + Medu, data = train)
  
  prediction <- predict(model, test, se.fit=TRUE)
  mean_pred <- mean((test$G3 - prediction$fit)^2)
  #higher_std <- mean(((test$G3 - model$fit) + 2 * model$se.fit)^2)
  #lower_std <- mean(((test$G3 - model$fit) - 2 * model$se.fit)^2)
  mean_cv_pred_err <- c(mean_cv_pred_err, c(mean_pred))
}
mean(mean_cv_pred_err)
```
Bład wzrósł, 


Następnie postanowiliśmy poprawić rezultat naszej predykcji, wykorzystując do tego funkcje sklejane:
Postanowiliśmy znaleźć najbardziej pasującą liczbę stopni swobody przy użyciu walidacji krzyżowej:
```{r}
#failures_max <- 7
#higherYes_max <- 7
#studtytime_max <- 7
#Dalc_max <- 7
#Medu_max <- 7
#
#for (d_1 in 4:failures_max){
#  for (d_2 in 4:higherYes_max){
#    for (d_3 in 4:studtytime_max){
#      for (d_4 in 4:Dalc_max){
#        for (d_5 in 4:Medu_max){
#          mean_cv_pred_err <- c()
#          for (cv_iter in 1:k){
#            train <- dataset[folds != cv_iter,]
#            test <- dataset[folds == cv_iter,]
#            
#            model <- lm(G3 ~ bs(failures, df = d_1) + bs(higherYes, df = d_2) + bs(studytime, df = d_3) + bs(Dalc, df=d_4) + bs(Medu, df=d_5) #, data = train)
#            
#            mean_cv_pred_err <- c(mean_cv_pred_err, c(mean((test$G3 - predict(model, test))^2)))
#          }
#          results[nrow(results) +1,] = c(d_1, d_2, d_3, d_4, d_5, mean(mean_cv_pred_err))
#        }
#      }
#    }
#  }
#}
```

```{r}
#results[order(results$test_error, decreasing = FALSE),]
```


Postanowiliśmy wykorzystać splajny naturalne:
```{r}
set.seed(2)
k <- 5
folds <- sample(1:k, nrow(dataset), replace=TRUE)

mean_cv_pred_err <- c()
for (cv_iter in 1:k){
  train <- dataset[folds != cv_iter,]
  test <- dataset[folds == cv_iter,]
  
  model <- lm(G3 ~ ns(failures) + ns(higherYes) + ns(studytime) + ns(Dalc) + ns(Medu) , data = train)
  
  prediction <- predict(model, test, se.fit=TRUE)
  mean_pred <- mean((test$G3 - prediction$fit)^2)
  mean_cv_pred_err <- c(mean_cv_pred_err, c(mean_pred))
}
mean(mean_cv_pred_err)
```

```{r}
summary(lm(G3 ~ ns(failures) + ns(higherYes) + ns(studytime) + ns(Dalc) + ns(Medu) , data = dataset))
```

```{r}
set.seed(2)
k <- 5
folds <- sample(1:k, nrow(dataset), replace=TRUE)

mean_cv_pred_err <- c()
for (cv_iter in 1:k){
  train <- dataset[folds != cv_iter,]
  test <- dataset[folds == cv_iter,]
  
  model <- lm(G3 ~ bs(failures) + bs(higherYes) + bs(studytime) + bs(Dalc) + bs(Medu) , data = train)
  
  prediction <- predict(model, test, se.fit=TRUE)
  mean_pred <- mean((test$G3 - prediction$fit)^2)
  mean_cv_pred_err <- c(mean_cv_pred_err, c(mean_pred))
}
mean(mean_cv_pred_err)
```

```{r}
summary(lm(G3 ~ bs(failures) + bs(higherYes) + bs(studytime) + bs(Dalc) + bs(Medu) , data = dataset))
```
W obu przypadkach błędy były większe niż w przypadku regularyzowanej regresji liniowej.

```{r}
medv_tree <- tree(G3 ~ ., data = dataset)
summary(medv_tree)
```

```{r}
plot(medv_tree)
text(medv_tree)
```
```{r}
set.seed(2)
k <- 5
folds <- sample(1:k, nrow(dataset), replace=TRUE)

mean_cv_pred_err <- c()
for (cv_iter in 1:k){
  train <- dataset[folds != cv_iter,]
  test <- dataset[folds == cv_iter,]
  
  model <- tree(G3 ~ ., data = dataset)
  
  prediction <- predict(model, test)
  mean_pred <- mean((test$G3 - prediction)^2)
  mean_cv_pred_err <- c(mean_cv_pred_err, c(mean_pred))
}
mean(mean_cv_pred_err)
```
Wykorzystanie drzewa regresyjnego jeszcze bardziej poprawiło błąd tesowy.

Chcieliśmy również wypróbować metody prunningu:
```{r}
medv_cv <- cv.tree(medv_tree)
plot(medv_cv$size, medv_cv$dev, type = "b")
```
```{r}
medv_pruned <- prune.tree(medv_tree, best = 3)
plot(medv_pruned)
text(medv_pruned)
```
```{r}
set.seed(2)
k <- 5
folds <- sample(1:k, nrow(dataset), replace=TRUE)

mean_cv_pred_err <- c()
for (cv_iter in 1:k){
  train <- dataset[folds != cv_iter,]
  test <- dataset[folds == cv_iter,]
  
  model <- prune.tree(tree(G3 ~ ., data = dataset), best=3)
  
  prediction <- predict(model, test)
  mean_pred <- mean((test$G3 - prediction)^2)
  mean_cv_pred_err <- c(mean_cv_pred_err, c(mean_pred))
}
mean(mean_cv_pred_err)
```
Niestety, przycinanie drzew do maksymalnej głębokości 3, nie dało oczekiwanego rezultatu.